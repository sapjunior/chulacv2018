{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2110443 - Computer Vision (2018/1)\n",
    "## Lab 5 - Simple Object Recognition\n",
    "In this lab, we will learn to use useful  handcrafted features to recognize objects in the provied images. This notebook includes both coding and written questions. Please hand in this notebook file with all outputs and your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Collaboration is encouraged in this course.</b> You must turn in <b>your own write ups</b> of all problems. If you collaborate with others, you must put the names and ids of the students you worked with in below block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaboration List:\n",
    "- ...\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import random as rng\n",
    "from matplotlib import pyplot as plt\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML,clear_output\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1 - A Letter from Nowhere (Special Assignment)\n",
    "We got multiple mysterious letters from nowhere. Here is one of them. As a computer engineering student, can you decipher this letter before unthinkable things happen? Your task is to write a program to <b>automate deciphering this image.</b> We hope that everything is going well after you decipher this secret. Please hand in the <b>image recognition output (put the recognition result on image)</b> and decoded message. \n",
    "\n",
    "<b>Basic Guidance:<b>\n",
    "1. Extract each digit from image & filter out the outlier.\n",
    "2. Group the recognized results into digit groups.\n",
    "3. Use <b>your own designed feature</b> to classify between 0 and 1 \n",
    "4. Write a function to decrypt them\n",
    " \n",
    "<b>Hints:</b>\n",
    "- You can use knowledge from previous lecture to improve the segmentation result by apply <b>?</b> on input image\n",
    "- Specific threshold based on shape properties or simple morphological operations can be use to keep only potential contours\n",
    "- This <a href=\"https://docs.opencv.org/3.4.2/d7/d4d/tutorial_py_thresholding.html\">page</a> <b>may</b> contains useful information!\n",
    "- What is the different feature between non-overlaped and overlapped stroke?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Describe how your algorithm work here (Thai or English).\n",
    "'''\n",
    "1. ..\n",
    "2. ..\n",
    "..\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Letter Image in \"assets/Lab5-LetterFromNowhere.jpg\" ###\n",
    "### FILL HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maker Based Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will learn how to apply a classic segmentation algorithm named <b>watershed</b>. This algorithm can be used to detect and extract objects in images that are <b>touching and/or overlapping</b> (like RBC in previous Lab!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that we want to build an application to count coins value from the following image. First of all, we need to segment each coins in to a individual connected component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleCoinImage = cv2.imread(\"assets/Lab5-SampleCoins.jpg\")\n",
    "sampleCoinGray = cv2.cvtColor(sampleCoinImage,cv2.COLOR_BGR2GRAY)\n",
    "tempImage = cv2.cvtColor(sampleCoinImage,cv2.COLOR_BGR2RGB)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title(\"Thai Coins\")\n",
    "plt.imshow(tempImage)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following blocks try to use edge information from previous lecture to seperate coins image into individual coin.\n",
    "Standard Sobel operator is used to extract edge information and only strong edge pixels are kept based on defined threshold. Try to adjust the edge threshold value to seperate each coin from others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSobelEdge(thresholdEdgeVal):\n",
    "    sobelX = np.uint8(np.absolute(cv2.Sobel(sampleCoinGray,cv2.CV_64F,1,0,ksize=3)))\n",
    "    sobelY = np.uint8(np.absolute(cv2.Sobel(sampleCoinGray,cv2.CV_64F,0,1,ksize=3)))\n",
    "    sobelXY = (sobelX + sobelY) > thresholdEdgeVal\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(\"Edge Image\")\n",
    "    plt.imshow(sobelXY,cmap='gray')\n",
    "    plt.show()\n",
    "interact(extractSobelEdge, thresholdEdgeVal=widgets.IntSlider(min=0,max=255,step=1,value=1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from the above threshold edge map, even we selects a precise edge threshold, we cannot segment connected coins into individual one. In order to use watershed algorithm, markers muse be place at the corresponded location of the objects in our image. The markers can be either manual define or calculate from various image processing techniques. We will start from using <a href=\"https://docs.opencv.org/3.4.2/d7/d4d/tutorial_py_thresholding.html\">automatic Otsu thesholding</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleCoinGray = cv2.blur(sampleCoinGray,(15,15))\n",
    "_,thresholdCoinImage = cv2.threshold(sampleCoinGray,0,255,cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "plt.imshow(thresholdCoinImage, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then apply <a href=\"https://docs.opencv.org/3.4.2/d7/d1b/group__imgproc__misc.html#ga8a0b7fdfcb7a13dde018988ba3a43042\">distance transform</a> to the thresholding output. Distance transform calculates the approximate or precise distance from every binary image pixel to the nearest zero pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distanceTransformOutput = cv2.distanceTransform(thresholdCoinImage,cv2.DIST_L2,3)\n",
    "plt.imshow(distanceTransformOutput,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick only pixel which is greater than 45% of max distance as potential coin markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxDistance = np.max(distanceTransformOutput)\n",
    "print('Max Distance:',maxDistance)\n",
    "roughMarkerPixel = np.uint8(distanceTransformOutput > 0.45 * maxDistance)\n",
    "backgroundMask = np.invert(roughMarkerPixel)\n",
    "plt.title('Potential Coins Location')\n",
    "plt.imshow(roughMarkerPixel, cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title('Background Pixel')\n",
    "plt.imshow(backgroundMask, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labeling each connected component with its unique label number by using <a href=\"https://docs.opencv.org/3.4.2/d3/dc0/group__imgproc__shape.html#gaedef8c7340499ca391d459122e51bef5\">cv2.connectedComponents</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, markers = cv2.connectedComponents(roughMarkerPixel)\n",
    "# Add 1 to make the marker label start with 1\n",
    "markers = markers+1\n",
    "\n",
    "# Mark background pixel with 0\n",
    "markers[np.where(backgroundMask==255)] = 0\n",
    "print('Min Label:',np.min(markers),'Max Label:',np.max(markers))\n",
    "\n",
    "plt.imshow(markers,cmap='rainbow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply <a href=\"https://docs.opencv.org/3.4.2/d7/d1b/group__imgproc__misc.html#ga3267243e4d3f95165d55a618c65ac6e1\">watershed algorithm</a> using created marker and input coins image. Is there any missing coin? Why? Fix the bug and state your instruction in below block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "How to find the missing coin?\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputImage = sampleCoinImage.copy()\n",
    "borderImage = sampleCoinImage.copy()\n",
    "markers = cv2.watershed(outputImage,markers)\n",
    "\n",
    "# -1 is border pixel  \n",
    "print('Min Label:',np.min(markers),'Max Label:',np.max(markers))\n",
    "\n",
    "# 1 is background \n",
    "for outputMarkerIdx in range(2,np.max(markers+1)):\n",
    "    color = (rng.randint(0,256), rng.randint(0,256), rng.randint(0,256))\n",
    "    outputImage[np.where(markers == outputMarkerIdx)] = color\n",
    "outputImage = cv2.cvtColor(outputImage,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Segmentation Result')\n",
    "plt.imshow(outputImage)\n",
    "plt.show()\n",
    "\n",
    "borderImage[np.where(markers == -1)] = (0,255,0)\n",
    "borderImage = cv2.cvtColor(borderImage,cv2.COLOR_BGR2RGB)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Segmentation Boundary')\n",
    "plt.imshow(borderImage)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2 - Coin Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above section, we can gracefully segment overlpping coins! Your today task is to implement a program to count total coin value from test images provided in assets folder. Please include two of your own test images by using smartphone camera. The images should show robustness of your designed alogorithm such as overlapping coins, tilt camera angle or shadow handling. (Optional) You will get extra points if you can use <b>same parameters</b> for all test images. Don't forget to show your work in step in below block.\n",
    "\n",
    "<b>Remark : </b>\n",
    "- There are eight test images in assets folder. Please hand in the recognition result in image files. \n",
    "- You must not count Yen Coin!!!\n",
    "\n",
    "<b>Basic Guidance:<b>\n",
    "1. Segment each coin into individual connected component and find the bounding box which can enclose those connect components by <a href=\"https://docs.opencv.org/3.4.2/d3/dc0/group__imgproc__shape.html#gacb413ddce8e48ff3ca61ed7cf626a366\">cv2.boundingRect</a>\n",
    "2. For each component, extract <b>useful features</b> (you have to design this by youself).\n",
    "3. Classify those components into Thai Coin classes.  \n",
    "\n",
    "<b>Hints:</b>\n",
    "- How do to discard noise/fill small hole from segmentation mask output?\n",
    "- From the beginning of this class, you had learned many potential image features, such as edge, color, contour and shape. Use them wisely.\n",
    "- Internet is your friend. You can search for relavent research papers and use their algorithm, but you must <b>give proper credits</b> by citing them in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Describe how your algorithm work here (Thai or English). You can provide any visualization if you want.\n",
    "'''\n",
    "1. ..\n",
    "2. ..\n",
    "..\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FILL HERE ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
